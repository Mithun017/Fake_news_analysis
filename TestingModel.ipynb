{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "try:\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    os.makedirs('dataset', exist_ok=True)  \n",
    "    subprocess.run(['kaggle', 'datasets', 'download', '-d', 'clmentbisaillon/fake-and-real-news-dataset', '-p', 'dataset'], check=True)\n",
    "    \n",
    "    # Unzip the downloaded dataset\n",
    "    with zipfile.ZipFile('dataset/fake-and-real-news-dataset.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('dataset')\n",
    "    print(\"Dataset downloaded and extracted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"Please ensure Kaggle API is set up (kaggle.json in ~/.kaggle/) and try again.\")\n",
    "    print(\"Alternatively, download Fake.csv and True.csv manually from: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\")\n",
    "    exit()\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    fake_data = pd.read_csv('dataset/Fake.csv')\n",
    "    true_data = pd.read_csv('dataset/True.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset files not found. Please ensure Fake.csv and True.csv are in the 'dataset' directory.\")\n",
    "    exit()\n",
    "\n",
    "# Add labels: 1 for fake, 0 for real\n",
    "fake_data['label'] = 1\n",
    "true_data['label'] = 0\n",
    "\n",
    "# Combine datasets\n",
    "data = pd.concat([fake_data, true_data], ignore_index=True)\n",
    "print(\"Dataset Overview:\")\n",
    "print(data.head())\n",
    "print(\"Dataset Size:\", data.shape)\n",
    "print(\"Class Distribution:\", data['label'].value_counts())\n",
    "\n",
    "# Clean and preprocess text for TF-IDF: lowercase, remove special characters, lemmatize\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove special characters\n",
    "    # Tokenize and lemmatize\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Statistical model preprocessing: TF-IDF\n",
    "# Use TF-IDF for feature extraction, limiting to 5000 features to reduce dimensionality\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(data['text'])\n",
    "y = data['label'].values\n",
    "\n",
    "# Deep learning preprocessing: BERT tokenizer\n",
    "# Tokenize text for BERT, limiting sequence length to 128 for computational efficiency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def encode_text(texts, max_length=128):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "X_bert = encode_text(data['text'])\n",
    "y_bert = tf.convert_to_tensor(data['label'].values)\n",
    "\n",
    "# Train-test split: 80-20 split for both models\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "X_train_bert, X_test_bert = {\n",
    "    'input_ids': X_bert['input_ids'][:int(0.8*len(data))],\n",
    "    'attention_mask': X_bert['attention_mask'][:int(0.8*len(data))]\n",
    "}, {\n",
    "    'input_ids': X_bert['input_ids'][int(0.8*len(data)):],\n",
    "    'attention_mask': X_bert['attention_mask'][int(0.8*len(data)):]\n",
    "}\n",
    "y_train_bert, y_test_bert = y_bert[:int(0.8*len(data))], y_bert[int(0.8*len(data)):]\n",
    "\n",
    "baseline = LogisticRegression(max_iter=1000)\n",
    "baseline.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_baseline = baseline.predict(X_test_tfidf)\n",
    "baseline_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1-Score': f1_score(y_test, y_pred_baseline)\n",
    "}\n",
    "print(\"\\nBaseline (Logistic Regression) Metrics:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "nb_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_nb),\n",
    "    'Precision': precision_score(y_test, y_pred_nb),\n",
    "    'Recall': recall_score(y_test, y_pred_nb),\n",
    "    'F1-Score': f1_score(y_test, y_pred_nb)\n",
    "}\n",
    "print(\"\\nNaive Bayes Metrics:\")\n",
    "for metric, value in nb_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "bert_model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Train BERT (3 epochs for demonstration; increase to 5-10 for better performance on GPU)\n",
    "bert_model.fit(X_train_bert, y_train_bert, epochs=3, batch_size=16, validation_data=(X_test_bert, y_test_bert))\n",
    "\n",
    "# Evaluate BERT\n",
    "logits = bert_model.predict(X_test_bert)\n",
    "y_pred_bert = tf.argmax(logits.logits, axis=1).numpy()\n",
    "bert_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test_bert, y_pred_bert),\n",
    "    'Precision': precision_score(y_test_bert, y_pred_bert),\n",
    "    'Recall': recall_score(y_test_bert, y_pred_bert),\n",
    "    'F1-Score': f1_score(y_test_bert, y_pred_bert)\n",
    "}\n",
    "print(\"\\nBERT Metrics:\")\n",
    "for metric, value in bert_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# 9. Performance Analysis & Visualization\n",
    "# Confusion matrices for all models\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "cm_bert = confusion_matrix(y_test_bert, y_pred_bert)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Baseline: Logistic Regression')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', ax=axes[1], cmap='Blues')\n",
    "axes[1].set_title('Naive Bayes')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "sns.heatmap(cm_bert, annot=True, fmt='d', ax=axes[2], cmap='Blues')\n",
    "axes[2].set_title('BERT')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for metrics comparison\n",
    "metrics_df = pd.DataFrame([baseline_metrics, nb_metrics, bert_metrics], index=['Logistic Regression', 'Naive Bayes', 'BERT'])\n",
    "metrics_df.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(title='Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)  \n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def download_kaggle_dataset():\n",
    "    \"\"\"Automatically download the Fake and Real News dataset from Kaggle.\"\"\"\n",
    "    dataset_name = \"clmentbisaillon/fake-and-real-news-dataset\"\n",
    "    try:\n",
    "        os.system(f\"kaggle datasets download -d {dataset_name} -p ./\")\n",
    "        \n",
    "        zip_path = \"./fake-and-real-news-dataset.zip\"\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(\"Dataset zip file not downloaded correctly. Check Kaggle API setup.\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./dataset\")\n",
    "        \n",
    "        fake_df = pd.read_csv(\"./dataset/Fake.csv\")\n",
    "        true_df = pd.read_csv(\"./dataset/True.csv\")\n",
    "        \n",
    "        fake_df['label'] = 0  # Fake news\n",
    "        true_df['label'] = 1  # Real news\n",
    "        \n",
    "        df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "        \n",
    "        df = df[['text', 'label']].dropna()\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Downloading and loading dataset...\")\n",
    "try:\n",
    "    df = download_kaggle_dataset()\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text_statistical(text):\n",
    "    \"\"\"Preprocess text for statistical models (bag-of-words/TF-IDF).\"\"\"\n",
    "    try:\n",
    "    \n",
    "        text = text.lower()\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in string.punctuation and word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "df['text_processed'] = df['text'].apply(preprocess_text_statistical)\n",
    "\n",
    "df = df[df['text_processed'] != '']\n",
    "print(f\"Dataset shape after preprocessing: {df.shape}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_processed'], df['label'], test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "print(\"Training baseline Naive Bayes model...\")\n",
    "vectorizer_bow = CountVectorizer(max_features=5000)\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "baseline_model = MultinomialNB()\n",
    "baseline_model.fit(X_train_bow, y_train)\n",
    "y_pred_baseline = baseline_model.predict(X_test_bow)\n",
    "\n",
    "baseline_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1-Score': f1_score(y_test, y_pred_baseline)\n",
    "}\n",
    "print(\"Baseline (Naive Bayes) Metrics:\", baseline_metrics)\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "lr_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'Precision': precision_score(y_test, y_pred_lr),\n",
    "    'Recall': recall_score(y_test, y_pred_lr),\n",
    "    'F1-Score': f1_score(y_test, y_pred_lr)\n",
    "}\n",
    "print(\"Logistic Regression Metrics:\", lr_metrics)\n",
    "\n",
    "print(\"Training BERT model...\")\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading BERT model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "train_dataset = NewsDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = NewsDataset(X_test, y_test, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "bert_model.train()\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "bert_model.eval()\n",
    "y_pred_bert = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        y_pred_bert.extend(preds.cpu().numpy())\n",
    "\n",
    "bert_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_bert),\n",
    "    'Precision': precision_score(y_test, y_pred_bert),\n",
    "    'Recall': recall_score(y_test, y_pred_bert),\n",
    "    'F1-Score': f1_score(y_test, y_pred_bert)\n",
    "}\n",
    "print(\"BERT Metrics:\", bert_metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Baseline (Naive Bayes)': baseline_metrics,\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'BERT': bert_metrics\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_df.plot(kind='bar')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Models')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_performance_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_lr.png')\n",
    "plt.show()\n",
    "\n",
    "cm_bert = confusion_matrix(y_test, y_pred_bert)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title('Confusion Matrix - BERT')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_bert.png')\n",
    "plt.show()\n",
    "\n",
    "metrics_df.to_csv('model_performance.csv')\n",
    "print(\"Results and visualizations saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e683da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
